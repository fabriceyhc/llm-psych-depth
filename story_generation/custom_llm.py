import asyncio
import json
import sys
import websockets

from typing import Any, List, Mapping, Optional
from langchain.callbacks.manager import CallbackManagerForLLMRun
from langchain.llms.base import LLM

class CustomLLM(LLM):

    URI: str
    
    @property
    def _llm_type(self) -> str:
        return "oobabooga/text-generation-webui"

    async def _run(self, context):
        request = {
            'prompt': context,
            'max_new_tokens': 2048,
            'auto_max_new_tokens': False,
            'max_tokens_second': 0,

            # Generation params. If 'preset' is set to different than 'None', the values
            # in presets/preset-name.yaml are used instead of the individual numbers.
            'preset': 'None',
            'do_sample': True,
            'temperature': 0.7,
            'top_p': 0.1,
            'typical_p': 1,
            'epsilon_cutoff': 0,  # In units of 1e-4
            'eta_cutoff': 0,  # In units of 1e-4
            'tfs': 1,
            'top_a': 0,
            'repetition_penalty': 1.18,
            'repetition_penalty_range': 0,
            'top_k': 40,
            'min_length': 0,
            'no_repeat_ngram_size': 0,
            'num_beams': 1,
            'penalty_alpha': 0,
            'length_penalty': 1,
            'early_stopping': False,
            'mirostat_mode': 0,
            'mirostat_tau': 5,
            'mirostat_eta': 0.1,
            'grammar_string': '',
            'guidance_scale': 1,
            'negative_prompt': '',

            'seed': -1,
            'add_bos_token': True,
            'truncation_length': 2048,
            'ban_eos_token': False,
            'custom_token_bans': '',
            'skip_special_tokens': True,
            'stopping_strings': []
        }

        async with websockets.connect(self.URI, ping_interval=None) as websocket:
            await websocket.send(json.dumps(request))

            collected_text = "" # context  # Start with the initial context

            while True:
                incoming_data = await websocket.recv()
                incoming_data = json.loads(incoming_data)

                event = incoming_data['event']
                if event == 'text_stream':
                    collected_text += incoming_data['text']
                elif event == 'stream_end':
                    return collected_text

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any
    ) -> str:
        return asyncio.run(self._run(prompt))

if __name__ == '__main__':
    # this URI is generated by running `/data1/llm-psych-depth/text-generation-webui/start_linux.sh`
    # NOTE: it changes every time the oobabooga server is started unfortunately...
    # public UI url: https://e86faac610b22eab21.gradio.live
    URI = 'wss://log-assessed-degree-substitute.trycloudflare.com/api/v1/stream'
    llm = CustomLLM(URI=URI)
    prompt = "Write a short and impactful story about loss and motherhood."
    output = llm(prompt)
    print(output)